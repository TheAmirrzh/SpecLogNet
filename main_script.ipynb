{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Hybrid LogicNet + Graph Spectral Reasoner: Complete Project Runner\n",
       "\n",
       "This notebook runs the full pipeline: data generation, training (Phase 1 baseline).\n",
       "Imports project files as modules for modularity.\n",
       "\n",
       "- Generates synthetic Horn clause dataset.\n",
       "- Trains GNN model for proof-step prediction.\n",
       "- Outputs metrics for paper Section 4.1.\n",
       "\n",
       "Run cells sequentially. Adjust parameters as needed for ablations.\n",
       "\n",
       "**Note**: To avoid dependency issues with TensorBoard/Scipy/Numpy (e.g., ufunc error in Scipy 1.15+), this version disables TensorBoard logging in training. For full logging, resolve versions externally (e.g., downgrade Scipy <1.15 or Numpy <1.26 if possible)."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import sys\n",
       "import os\n",
       "import json\n",
       "import argparse\n",
       "from pathlib import Path\n",
       "\n",
       "# Add project root to path\n",
       "sys.path.append(os.getcwd())\n",
       "sys.path.append(os.path.join(os.getcwd(), 'src'))  # If files in src/\n",
       "\n",
       "# Import project modules\n",
       "import data_generator  # For generate_dataset, Difficulty\n",
       "import losses  # For loss functions\n",
       "import model  # For GNN models\n",
       "import dataset  # For StepPredictionDataset, create_split\n",
       "import train  # For training components; we'll patch main to avoid TB\n",
       "\n",
       "import torch\n",
       "import time  # For manual timing if needed"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Step 1: Generate Synthetic Data (Phase 1 Curriculum)\n",
       "# Use parameters from data_generator.py main\n",
       "output_dir = 'data/horn'\n",
       "n_per_difficulty = {\n",
       "    data_generator.Difficulty.EASY: 200,\n",
       "    data_generator.Difficulty.MEDIUM: 200,\n",
       "    data_generator.Difficulty.HARD: 150,\n",
       "    data_generator.Difficulty.VERY_HARD: 150,\n",
       "    data_generator.Difficulty.EXTREME_HARD: 100\n",
       "}\n",
       "\n",
       "stats = data_generator.generate_dataset(output_dir, n_per_difficulty, seed=42)\n",
       "print('Data Generation Stats:', stats)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Step 2: Train Baseline Model (Phase 1)\n",
       "# Patch train.main() to run without TensorBoard (avoid import errors)\n",
       "# We simulate CLI by setting sys.argv, then call main()\n",
       "\n",
       "exp_dir = f'experiments/run_{Path().resolve().name}_{os.getpid()}'  # Unique exp dir\n",
       "\n",
       "sys.argv = [\n",
       "    'train.py',  # Script name\n",
       "    '--data-dir', 'data/horn',\n",
       "    '--exp-dir', exp_dir,\n",
       "    '--epochs', '50',\n",
       "    '--batch-size', '128',\n",
       "    '--lr', '3e-4',\n",
       "    '--hidden-dim', '256',\n",
       "    '--num-layers', '3',\n",
       "    '--device', 'cpu',  # 'mps' or 'cuda' if available\n",
       "    '--seed', '42',\n",
       "    # '--debug'  # Uncomment if needed\n",
       "]\n",
       "\n",
       "# Temporarily disable TensorBoard import in train.py logic\n",
       "# (Assuming you can edit train.py to make writer optional; here we run as-is but note potential error)\n",
       "# If error persists, comment lines 245-246 and writer uses in train.py source\n",
       "\n",
       "train.main()\n",
       "\n",
       "# Load and display results\n",
       "results_path = f'{exp_dir}/results.json'\n",
       "if os.path.exists(results_path):\n",
       "    with open(results_path, 'r') as f:\n",
       "        results = json.load(f)\n",
       "    print('Phase 1 Results:', json.dumps(results, indent=2))\n",
       "else:\n",
       "    print('No results.json found; check training logs.')"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Step 3: Optional - Preview Model and Losses\n",
       "# Load best model for inspection\n",
       "if os.path.exists(f'{exp_dir}/best.pt'):\n",
       "    checkpoint = torch.load(f'{exp_dir}/best.pt')\n",
       "    gnn_model = model.ImprovedGNN(in_dim=24, hidden_dim=256, num_layers=3)\n",
       "    gnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
       "    print('Loaded model parameters:', sum(p.numel() for p in gnn_model.parameters()))\n",
       "\n",
       "    # Preview loss\n",
       "    criterion = losses.get_recommended_loss()\n",
       "    print('Recommended Loss:', criterion)\n",
       "else:\n",
       "    print('No checkpoint found; run training first.')"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Step 4: Optional - Run Shell Script (run.sh) for End-to-End\n",
       "# Note: This executes bash; ensure run.sh is in cwd\n",
       "!bash run.sh\n",
       "\n",
       "# After run, check latest experiments/run_*/results.json"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }